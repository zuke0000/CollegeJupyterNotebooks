{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "artistic-fraction",
   "metadata": {},
   "source": [
    "# Topic 2: Estimation and the Bayes Classifier\n",
    " \n",
    "## Associated Reading: Murphy 2.4, 2.5\n",
    "\n",
    "### 2.1 Parameterized probability\n",
    "In the above examples, we've been using something called a *probability table*, in which we explicitly enumerate all possible cases\n",
    "\n",
    "| y | P(Y=y)   |\n",
    "|------|------|\n",
    "|   Orange  | 0.25 |\n",
    "|   Kiwi    | 0.75 |\n",
    "\n",
    "This table is a valid way to specify the function from $y$ to its probability.  However, in general we'd like to be able to write these tables more succinctly, particularly when the number of possible values that $y$ could take becomes very large: indeed, it's reasonable to imagine that $Y$ may be a real-valued random variable, in which case there would be an infinite number of entries in the probability table (this actually breaks down in some sense, because the probability of drawing any particular real value from a distribution is zero: instead we think about probability densities.  More soon).  Instead of writing down tables of probabilities, we'll instead introduce a *parameterized* function that maps from a given outcome to a probability.  We'll work with a few of these distributions throughout the course, but we'll start with the simplest, and among the most useful.  \n",
    "\n",
    "### 2.2 The Bernoulli Distribution\n",
    "The Bernoulli distribution is the distribution that gives the probability of two mutually exclusive events.  The most common example of such a scenario is a coin flip, but, as we'll see, there are lots of other situations that can be modelled similarly.  The Bernoulli distribution has the *probability mass function* \n",
    "$$ P(Y=y) = \\mathrm{Bernoulli}(Y=y;\\theta) = \\theta^y (1-\\theta)^{1-y} $$\n",
    "where $y\\in\\{0,1\\}$ indicates \"success\" or \"failure\".  **For a fair coin, what should be the value of $\\theta$?  What is P(Y=1) and P(Y=0) for a fair coin?  What is the parameter that controls this function?**\n",
    "\n",
    "This isn't actually very different from a probability table. If we were to look at the table giving the probability of orange versus kiwi, we might encode \"draw an orange $\\equiv 1$, in which case the table could equivalently be written as $P(Y) = \\mathrm{Bernoulli}(Y;\\theta=0.25)$.  Just for clarity, it's worth evaluating this function for the values 1 and 0:\n",
    "$$\n",
    "P(Y=1) = \\theta^1 (1-\\theta)^0 = \\theta\n",
    "$$\n",
    "$$\n",
    "P(Y=0) = \\theta^0 (1-\\theta)^1 = 1-\\theta.\n",
    "$$\n",
    "Thus, the probability of drawing a 1 (or a heads, or an orange) is $\\theta$, and the probability of drawing a 0 (or a tails, or a kiwi) is $1-\\theta$.  Cool, but where does $\\theta$ come from?    \n",
    "\n",
    "### 2.3 Inferring $\\theta$\n",
    "Imagine a scenario: we are given a coin, and we want to determine whether it is fair (i.e. $\\theta=0.5$) or not.  Of course, we have access to flips from the coin (to begin with, let's imagine that we have the result of a single coin flip).  Bayes' theorem then would be\n",
    "$$\n",
    "P(\\theta|Y=y) = \\frac{P(Y=y|\\theta)P(\\theta)}{P(Y=y)},\n",
    "$$\n",
    "which is to say that if we have the result of some coin flips $Y=y$, we can *update* our belief in what $\\theta$ should be.  Note that using Bayes' theorem gives us a probability distribution over $\\theta$, which is real-valued!  Similarly, $P(\\theta)$, which is the prior is also a probability distribution over $\\theta$.  For a judicious choice of the prior distribution (a Beta distribution), it turns our that we can actually evaluate Bayes' theorem in closed form exactly for this problem.  However, doing full Bayesian inference of this type is usually pretty hard (methods for doing this would be the subject of a Bayesian Statistics class, although they will show up in CSCI 557: Deep Learning as well).  **For now, let's punt, by which I mean let's see if we can figure out a way to solve a simpler problem.**  In particular, we'll try to solve the problem of estimating just the maximum of the posterior distribution, the most probable value of $\\theta$ given the observed coin flip, which turns out to be much easier.\n",
    "\n",
    "First, let's start with the prior distribution $P(\\theta)$.  Recall that this encodes our belief in what $\\theta$ should be prior to having looked at any data.  It asks the question, \"do you trust the coin flipper?\"  A very easy assumption, and one that will greatly simplify this particular analysis is to say that *we have no idea:* before looking at the data, any value of $\\theta$ is as likely as any other.  The way that we can encode this assumption is to say that $P(\\theta)=c$, where $c$ is some constant.  Thus we now have that\n",
    "$$\n",
    "P(\\theta|Y=y) = \\frac{c P(Y=y|\\theta)}{P(Y=y)}.\n",
    "$$\n",
    "Here's an important fact that you'll want to remember for the rest of your life: scaling the function to be minimized by a constant doesn't make a difference in that maximum's location.  $c$ is not a function of $\\theta$, so it won't have any effect on determining which value of $\\theta$ maximizes the posterior probability.  However, we could also say the same thing about the denominator: it's not a function of $\\theta$, so it doesn't influence where the maximum of the posterior is with respect to $\\theta$.  Thus we could simply rewrite Bayes' theorem as:\n",
    "$$\n",
    "P(\\theta|Y=y) \\propto P(Y=y|\\theta),\n",
    "$$\n",
    "with the symbol $\\propto$ meaning 'proportional to' or equal up to a multiplicative constant.  For the purposes of finding the maximum, 'proportional to' is just as good as 'equal to.'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-racing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-1,1,101)\n",
    "y = x**2 + 1\n",
    "logy = np.log(y)\n",
    "plt.plot(x,y)\n",
    "plt.plot(x,logy)\n",
    "plt.show()\n",
    "print(np.argmin(y),np.argmin(logy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-queue",
   "metadata": {},
   "source": [
    "Now, all that's left is to specify the *likelihood*.  Recall that the likelihood is asking the question: if I assume a model, what's the probability of observing the data?  Specific to this problem, it's asking: if I know $\\theta$, how likely is it that I should have observed a heads?  Of course, we already know how to model that situation: that's just the Bernoulli distribution that we developed above.  For a single coin flip, that's\n",
    "$$\n",
    "P(\\theta|Y=y) \\propto P(Y=y|\\theta) = \\theta^y (1-\\theta)^{1-y}.\n",
    "$$\n",
    "Let's make it explicit that we want to find the most probable value of theta:\n",
    "\\begin{align}\n",
    "\\hat{\\theta} & = \\mathrm{argmax}_\\theta P(\\theta|Y=y) \\\\\n",
    "             & = \\mathrm{argmax}_\\theta P(Y=y|\\theta) \\\\\n",
    "             & = \\mathrm{argmax}_\\theta \\theta^y (1-\\theta)^{1-y}.\n",
    "\\end{align}\n",
    "This says that the most likely value of $\\theta$ is the one that makes the data most likely to have occurred.  Often times, this is called *maximum likelihood estimation*.  Now all we need to do is figure out how to maximize it!\n",
    "\n",
    "It turns out that it will be much easier to maximize the *logarithm* of the probability.  It will be clearer why this is easier in a moment; rest assured, since logarithms are a monotonic function (i.e. $x\\ge y \\iff \\ln x \\ge \\ln y$), taking the logarithm does not change the location of the minimum.  The log of the likelihood is \n",
    "$$\n",
    "\\ln P(Y=y|\\theta) = y \\ln \\theta + (1-y) \\ln (1-\\theta)\n",
    "$$\n",
    "Setting this equal to zero and taking the derivative with respect to theta yields\n",
    "$$\n",
    "\\frac{y}{\\theta} - \\frac{1-y}{1-\\theta} = 0 \\rightarrow \\theta = y.\n",
    "$$\n",
    "This is sort of a funny result: it says that if we flip a coin one time and it comes up heads, our best option is to assume that the next coin flip will also be heads, with 100% certainty.  This is, however, exactly what we asked of Bayes' theorem: by using an uninformative prior distribution, we said that $\\theta$ could be *any value with equal probability*.  What else do we have to go on then?  Just the data, and the data tells us that 100% of the time (so far), the coin comes up heads.  \n",
    "\n",
    "#### ASIDE: The influence of a prior distribution\n",
    "This is obviously not an ideal model of coin flips, and it perhaps the simplest case of overfitting imaginable.  We have two paths forward.\n",
    "\n",
    "First, we could specify a more sensible prior distribution on $\\theta$.  We don't need to go through the math on this, but the typical way forward would be to assume that $\\theta$ is distributed according to a [Beta distribution](https://en.wikipedia.org/wiki/Beta_distribution).   Setting such a prior can be viewed as including *synthetic observations*.  The resulting best estimate (called Maximum a Posteriori estimate or MAP) of $\\theta$ is given by\n",
    "$$\n",
    "\\theta = \\frac{y + M^+}{1 + M^+ + M^-},\n",
    "$$\n",
    "where $M^+$ is the number of synthetic heads, and $M^-$ is the number of synthetic tails.  As an example, if we assumed that we had taken ten synthetic coin flips, five of which were heads, and five of which were tails, and our *real* flip is a heads.  Then we have that\n",
    "$$\n",
    "\\theta = \\frac{ 1 + 5}{1 + 5 +5} = \\frac{6}{11}.\n",
    "$$\n",
    "Thus, our prior belief that the coin was fair (because we made our synthetic coin flips equal between heads and tails) keeps our one *new* coin flip from adjusting our belief in the value of $\\theta$ too much.\n",
    "\n",
    "Secondly, we could simply collect more data.\n",
    "\n",
    "**How about for two coin flips?** \n",
    "\n",
    "Inferring the degree of weightedness for a coin based on a single coin flip is tenuous at best: if we really want to know, we should probably flip the coin at least twice.  If we assume that the results of the two coin flips are not directly dependent upon one another (which is to say that they are *independent*), then we can simply write this as \n",
    "$$\n",
    "P(Y_1=y_1,Y_2=y_2|\\theta) = P(Y_1=y_1|\\theta) P(Y_2=y_2|\\theta)\n",
    "$$\n",
    "More generally, for an arbitrary number of coin flips $m$, we have that\n",
    "$$\n",
    "P(\\mathbf{y}|\\theta) = \\prod_{i=1}^m \\theta^{y_i} (1-\\theta)^{1-y_i}\n",
    "$$,\n",
    "where $\\mathbf{y}$ is a vector of length $m$ containing the results of our $m$ coin flips.\n",
    "\n",
    "While not easily done in general, for this simple problem we can actually plot this likelihood as a function of $\\theta$ for a simulated set of coin flips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fewer-container",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "import matplotlib.pyplot as plt\n",
    "# random coin flips\n",
    "y = np.random.randint(0,2,50)\n",
    "\n",
    "# Array of thetas\n",
    "theta = np.linspace(0,1,101)\n",
    "# Array to hold the posterior values of theta\n",
    "\n",
    "# Move the product inside the exponent to produce a sum\n",
    "post = theta**np.sum(y)*(1-theta)**np.sum(1-y)\n",
    "    \n",
    "# Plot the resulting function of theta\n",
    "plt.plot(theta,post)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-setting",
   "metadata": {},
   "source": [
    "Now, we just need to find the value of $\\theta$ that maximizes this function.  We do this exactly the same way as above: by taking the derivative of likelihood and setting it equal to zero.  Taking derivatives of *products* is hard (think about the product rule from calculus), and this is the real reason why we like working with the logarithm instead: it converts products to sums!  Taking the log is pretty easy:\n",
    "$$\n",
    "\\ln \\left[\\prod_{i=1}^m \\theta^{y_i} (1-\\theta)^{1-y_i}\\right] = \\sum_{i=1}^m y_i \\ln \\theta + (1-y_i)\\ln (1-\\theta)\n",
    "$$\n",
    "Note that this quantity, the logarithm of the Bernoulli distribution, is also called the *cross-entropy*.  We will return to it later.  Now, all that's left is to take the derivative\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta}\\sum_{i=1}^m y_i \\ln \\theta + (1-y_i)\\ln (1-\\theta) = \\left[\\sum_{i=1}^m \\frac{y_i}{\\theta} - \\frac{1-y_i}{1-\\theta}\\right],\n",
    "$$\n",
    "set it equal to zero and solve for $\\theta$\n",
    "$$\n",
    "\\theta = \\frac{\\sum_{i=1}^m y_i}{m}\n",
    "$$\n",
    "\n",
    "This specific result is called the Maximum Likelihood Estimator (MLE) for a Bernoulli model.  Given the data, it tells us what the most likely value for $\\theta$, or alternatively, whether the coin is loaded or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-berlin",
   "metadata": {},
   "source": [
    "## In-class exercise: Ham vs Spam\n",
    "One use of the naive Bayes classifier, which is still in practical use today, is as a spam filter.  Consider the corpus of text messages packaged with this homework, which are each labelled as either 'spam' or 'ham'.  In this case, naive Bayes utilizes a Bernoulli model that quantifies the probability of a given word given that the message is either spam or ham.  For example, investigating the text messages here, we find that the word *draw* shows up in spam 27 times, implying that\n",
    "$$P(X=\\mathrm{draw}|Y=\\mathrm{spam}) = \\frac{27}{25748} = \\frac{m_{draw,spam}}{m_{spam}},$$\n",
    "while in the case of ham, it shows up 5 times so\n",
    "$$P(X=\\mathrm{draw}|Y=\\mathrm{ham}) = \\frac{5}{67148} = \\frac{m_{draw,ham}}{m_{ham}}.$$\n",
    "Thus we see that the word 'draw' shows up with a much higher frequency in spam e-mails than in ham.\n",
    "\n",
    "While this is not particularly strong evidence on its own, we can create a powerful classifier by using the naive assumption in conjunction with all the words in a given message:\n",
    "$$ P(Y=\\mathrm{ham}|X=x) \\propto P(Y=\\mathrm{ham}) \\prod_{i=1}^n P(X_i=x_i|Y=\\mathrm{ham}), $$\n",
    "$$ P(Y=\\mathrm{spam}|X=x) \\propto P(Y=\\mathrm{spam}) \\prod_{i=1}^n P(X_i=x_i|Y=\\mathrm{spam}), $$\n",
    "where $x_i$ are the words in a given message. \n",
    "\n",
    "Your task is to write such a classifier.  I have taken the somewhat tedious step of parsing the data for you, yielding the variables *word_dictionary*, which contains the ham and spam counts for each word, as well as *training_labels*, which provides the spam/ham labels for each text message.  I have also parsed a set of test data: *test_messages* is a list, each entry containing another list of the words in the text message, as well as *test_labels* which contains the spam/ham label for each message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-laugh",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Maps from 'ham' or 'spam' strings to zero or one\n",
    "def mapper(s):\n",
    "    if s=='spam':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# Read in the text file\n",
    "f = open('SMSSpamCollection','r')\n",
    "lines = f.readlines()\n",
    "\n",
    "# Break out the test data\n",
    "test_lines = lines[:len(lines)//5]\n",
    "lines = lines[len(lines)//5:]\n",
    "\n",
    "# Instantiate the frequency dictionary and an array to\n",
    "# record whether the line is ham or spam\n",
    "word_dictionary = {}\n",
    "training_labels = np.zeros(len(lines),dtype=int)\n",
    "\n",
    "# Loop over all the training messages\n",
    "for i,l in enumerate(lines):\n",
    "    # Split into words\n",
    "    l = l.lower().split()\n",
    "    # Record the special first word which always ham or spam\n",
    "    if l[0]=='ham':\n",
    "        training_labels[i] = 1\n",
    "    # For each word in the message, record whether the message was ham or spam\n",
    "    for w in l[1:]:\n",
    "        # If we've never seen the word before, add a new dictionary entry\n",
    "        if w not in word_dictionary:\n",
    "            word_dictionary[w] = [1,1]\n",
    "        word_dictionary[w][mapper(l[0])] += 1\n",
    "        \n",
    "# Loop over the test messages\n",
    "test_labels = np.zeros(len(test_lines),dtype=int)\n",
    "test_messages = []\n",
    "for i,l in enumerate(test_lines):\n",
    "    l = l.lower().split()\n",
    "    if l[0]=='ham':\n",
    "        test_labels[i] = 1\n",
    "    test_messages.append(l)\n",
    "\n",
    "counts = np.array([v for v in word_dictionary.values()]).sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-bunny",
   "metadata": {},
   "source": [
    "Below, I have provided code skeletons. Your job is to make the code skeletons into an operational naive Bayes spam detector. (you may discard these skeletons if you would prefer to code this from scratch). Note that lines where you will need to change the code are marked with a '#!'.\n",
    "\n",
    "Your first task is train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-stake",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the prior P(Y=ham) ?\n",
    "ham_prior = #!\n",
    "spam_prior = #!\n",
    "\n",
    "# What are the class probabilities P(X=word|Y=ham) for each word?\n",
    "ham_likelihood = {}\n",
    "spam_likelihood = {}\n",
    "for key,val in word_dictionary.items():\n",
    "    ham_likelihood[key] =  #!\n",
    "    spam_likelihood[key] = #!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rural-senate",
   "metadata": {},
   "source": [
    "Your next task is to make predictions on a set of test examples which were held back from the training procedure (see *test_messages* variable).  For each of these messages, compute the ham and spam probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-equity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to hold the ham and spam posteriors\n",
    "posteriors = np.zeros((len(test_lines),2))\n",
    "\n",
    "# Loop over all the messages in the test set\n",
    "for i,m in enumerate(test_messages):\n",
    "    posterior_ham = 1.0\n",
    "    posterior_spam = 1.0\n",
    "    #! Don't forget to include the prior!\n",
    "    # Loop over all the words in each message\n",
    "    for w in m:\n",
    "        # #! What is the purpose of this try/except handler?\n",
    "        try:\n",
    "            posterior_ham *= 1 #!\n",
    "            posterior_spam *= 1 #!\n",
    "        except KeyError:\n",
    "            pass\n",
    "    \n",
    "    # Notice the normalization factor (denominator) \n",
    "    # to turn these into proper probabilities!\n",
    "    posteriors[i,0] = posterior_spam/(posterior_spam + posterior_ham)\n",
    "    posteriors[i,1] = posterior_ham/(posterior_spam + posterior_ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-fundamentals",
   "metadata": {},
   "source": [
    "Finally, **make a ham/spam prediction based on your posterior probabilities.  Compare these to the labels contained in test_labels.  Report the accuracy of your classifier as percentage correct.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-gardening",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
