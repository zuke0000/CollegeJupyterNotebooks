{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d364c136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "098a6d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc989296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\derrick\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\derrick\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\derrick\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\derrick\\anaconda3\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: click in c:\\users\\derrick\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\derrick\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "# install the nltk (natural language toolkit) library\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc96e5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "# sentences\n",
    "X_train = ['I love the book', \n",
    "           'This is a great book',\n",
    "          'The fit is great',\n",
    "          'I love the shoes']\n",
    "# topics of sentences\n",
    "y_train = ['books', 'books', 'clothings', 'clothings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1a2d99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de11778c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book</th>\n",
       "      <th>fit</th>\n",
       "      <th>great</th>\n",
       "      <th>is</th>\n",
       "      <th>love</th>\n",
       "      <th>shoes</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   book  fit  great  is  love  shoes  the  this\n",
       "0     1    0      0   0     1      0    1     0\n",
       "1     1    0      1   1     0      0    0     1\n",
       "2     0    1      1   1     0      0    1     0\n",
       "3     0    0      0   0     1      1    1     0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "vect.fit(X_train)\n",
    "# document term matrix. Matrix with the counts\n",
    "X_train_dtm = vect.transform(X_train)\n",
    "pd.DataFrame(X_train_dtm.toarray(), \n",
    "             columns=vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5613568",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = ['I like the book', \n",
    "          'Shoes are alright', \n",
    "          'I love the books',\n",
    "         'I lost a shoe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f1ee203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(X=X_train_dtm, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bd357b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['books', 'clothings', 'clothings', 'books'], dtype='<U9')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_dtm = vect.transform(X_test)\n",
    "nb_clf.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f0f4f8",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40043513",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Derrick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Derrick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Derrick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Derrick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Derrick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6adcff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93d1c6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book\n",
      "read\n"
     ]
    }
   ],
   "source": [
    "# initialize the stemmer\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('books'))\n",
    "print(stemmer.stem('reading'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c40f280",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = 'I love the books'\n",
    "words = word_tokenize(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d69213b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'love', 'the', 'book']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_words = [stemmer.stem(word) for word in words] # some nice python syntax\n",
    "stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63fa758f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i love the book'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4f9fbf",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92cc4427",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abae3937",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cff25b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# needs parts of speech\n",
    "lemmatizer.lemmatize('eats', pos='v') # v for verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b78b062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'love', 'the', 'books']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2387daf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'), ('love', 'VBP'), ('the', 'DT'), ('books', 'NNS')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(words) # tells if a word is a verb (VBP) or a noun (NNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1f7575e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parts of speech processing function\n",
    "def process_pos(pos):\n",
    "    if pos.startswith('J'): # adjective\n",
    "        return wordnet.ADJ\n",
    "    elif pos.startswith('V'): # verb\n",
    "        return wordnet.VERB\n",
    "    elif pos.startswith('N'): # noun\n",
    "        return wordnet.NOUN\n",
    "    elif pos.startswith('R'): #adverb\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eaeecf54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'), ('love', 'VBP'), ('the', 'DT'), ('books', 'NNS')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7dc452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_words = [lemmatizer.lemmatize(word, pos=process_pos(pos))\n",
    "                   for word,pos in nltk.pos_tag(words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44d2a6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = 'example of sentence removal of stopwords'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "879ee4d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'example sentence removal stopwords'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words\n",
    "\n",
    "words = word_tokenize(phrase)\n",
    "stripped_phrase = [word for word in words if word not in stop_words]\n",
    "' '.join(stripped_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fffdc7",
   "metadata": {},
   "source": [
    "## Punctuation Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c93e647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import string\n",
    "punctuation = [punc for punc in string.punctuation]\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "942a39ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'example sentence removal stopwords'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase = \"Hello! how are you?\"\n",
    "words = word_tokenize(phrase)\n",
    "stripped_phrease = [word for word in words if word not in punctuation]\n",
    "' '.join(stripped_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7fc9f8",
   "metadata": {},
   "source": [
    "## Yelp reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "630a326f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  stars\n",
       "0  My wife took me here on my birthday for breakf...      5\n",
       "1  I have no idea why some people give bad review...      5\n",
       "2  love the gyro plate. Rice is so good and I als...      4\n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...      5\n",
       "4  General Manager Scott Petello is a good egg!!!...      5"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/um-perez-alvaro/Data-Science-Practice/master/Data/yelp.csv'\n",
    "yelp = pd.read_csv(url)[['text','stars']]\n",
    "yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "18b03635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    3526\n",
       "5    3337\n",
       "3    1461\n",
       "2     927\n",
       "1     749\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.stars.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bd892ccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>Yes I do rock the hipster joints.  I dig this ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>Only 4 stars? \\n\\n(A few notes: The folks that...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>I'm not normally one to jump at reviewing a ch...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>Let's see...what is there NOT to like about Su...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>4-5 locations.. all 4.5 star average.. I think...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  stars\n",
       "9990  Yes I do rock the hipster joints.  I dig this ...      5\n",
       "9991  Only 4 stars? \\n\\n(A few notes: The folks that...      5\n",
       "9992  I'm not normally one to jump at reviewing a ch...      5\n",
       "9994  Let's see...what is there NOT to like about Su...      5\n",
       "9999  4-5 locations.. all 4.5 star average.. I think...      5"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter to 5- and 1- star reviews\n",
    "yelp = yelp.loc[yelp.stars.isin([1,5])]\n",
    "yelp.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b131cb35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\\n\\nDo yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I\\'ve ever had.  I\\'m pretty sure they only use ingredients from their garden and blend them fresh when you order it.  It was amazing.\\n\\nWhile EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  It was the best \"toast\" I\\'ve ever had.\\n\\nAnyway, I can\\'t wait to go back!'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = yelp.loc[yelp.stars==5].iloc[0].text\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a0c85ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"wife take birthday breakfast excellent weather perfect make sit outside overlook ground absolute pleasure waitress excellent food arrive quickly semi-busy saturday morning look like place fill pretty quickly early get good favor get bloody mary phenomenal simply best 've ever 'm pretty sure use ingredient garden blend fresh order amaze everything menu look excellent white truffle scramble egg vegetable skillet tasty delicious come 2 piece griddle bread amaze absolutely make meal complete best `` toast '' 've ever anyway ca n't wait go back\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process the text\n",
    "text = yelp.loc[0,'text']\n",
    "words = word_tokenize(text)\n",
    "words = [word.lower() for word in words]\n",
    "lemmatized_words = [lemmatizer.lemmatize(word,pos=process_pos(pos))\n",
    "                   for word,pos in nltk.pos_tag(words)\n",
    "                   if word not in stop_words and word not in punctuation\n",
    "                   ]\n",
    "' '.join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f466b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp['processed_text'] = yelp.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8eae1647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    words = [word.lower() for word in words]\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word,pos=process_pos(pos))\n",
    "                       for word,pos in nltk.pos_tag(words)\n",
    "                       if word not in stop_words and word not in punctuation\n",
    "                       ]\n",
    "    return(' '.join(lemmatized_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dbdf299e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"wife take birthday breakfast excellent weather perfect make sit outside overlook ground absolute pleasure waitress excellent food arrive quickly semi-busy saturday morning look like place fill pretty quickly early get good favor get bloody mary phenomenal simply best 've ever 'm pretty sure use ingredient garden blend fresh order amaze everything menu look excellent white truffle scramble egg vegetable skillet tasty delicious come 2 piece griddle bread amaze absolutely make meal complete best `` toast '' 've ever anyway ca n't wait go back\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ee54c6d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m yelp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43myelp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_text\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4324\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4328\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4329\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4330\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4331\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4332\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4431\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4432\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1082\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;66;03m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[0;32m   1080\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m-> 1082\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1137\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m   1132\u001b[0m         \u001b[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m         \u001b[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m         \u001b[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m         \u001b[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;66;03m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[1;32m-> 1137\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1138\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1140\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1141\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1144\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Input \u001b[1;32mIn [35]\u001b[0m, in \u001b[0;36mprocess_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      2\u001b[0m words \u001b[38;5;241m=\u001b[39m word_tokenize(text)\n\u001b[0;32m      3\u001b[0m words \u001b[38;5;241m=\u001b[39m [word\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[0;32m      4\u001b[0m lemmatized_words \u001b[38;5;241m=\u001b[39m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word,pos\u001b[38;5;241m=\u001b[39mprocess_pos(pos))\n\u001b[1;32m----> 5\u001b[0m                    \u001b[38;5;28;01mfor\u001b[39;00m word,pos \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m                    \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words \u001b[38;5;129;01mand\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m punctuation\n\u001b[0;32m      7\u001b[0m                    ]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(lemmatized_words))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py:166\u001b[0m, in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;124;03mUse NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03mtag the given list of tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m:rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    165\u001b[0m tagger \u001b[38;5;241m=\u001b[39m _get_tagger(lang)\n\u001b[1;32m--> 166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_pos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py:123\u001b[0m, in \u001b[0;36m_pos_tag\u001b[1;34m(tokens, tagset, tagger, lang)\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens: expected a list of strings, got a string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 123\u001b[0m     tagged_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtagger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tagset:  \u001b[38;5;66;03m# Maps to the specified tagset.\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m lang \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\perceptron.py:187\u001b[0m, in \u001b[0;36mPerceptronTagger.tag\u001b[1;34m(self, tokens, return_conf, use_tagdict)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tag:\n\u001b[0;32m    186\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_features(i, word, context, prev, prev2)\n\u001b[1;32m--> 187\u001b[0m     tag, conf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_conf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m output\u001b[38;5;241m.\u001b[39mappend((word, tag, conf) \u001b[38;5;28;01mif\u001b[39;00m return_conf \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (word, tag))\n\u001b[0;32m    190\u001b[0m prev2 \u001b[38;5;241m=\u001b[39m prev\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\perceptron.py:65\u001b[0m, in \u001b[0;36mAveragedPerceptron.predict\u001b[1;34m(self, features, return_conf)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     64\u001b[0m     weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[feat]\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m label, weight \u001b[38;5;129;01min\u001b[39;00m weights\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     66\u001b[0m         scores[label] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m value \u001b[38;5;241m*\u001b[39m weight\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Do a secondary alphabetic sort, for stability\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "yelp['processed_text'] = yelp.text.apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e85a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040d25b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB # or any other classifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c983720c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = yelp.processed_text\n",
    "y = yelp.stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ca0bb396",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mX\u001b[49m\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d5519e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f948d492",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(max_features = 1000, ngram_range=(1,2))), # idk what ngram range is. less features better for TfidVectorizer\n",
    "    ('clf', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "72b6649f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                 TfidfVectorizer(max_features=1000, ngram_range=(1, 2))),\n",
       "                (&#x27;clf&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                 TfidfVectorizer(max_features=1000, ngram_range=(1, 2))),\n",
       "                (&#x27;clf&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_features=1000, ngram_range=(1, 2))</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 TfidfVectorizer(max_features=1000, ngram_range=(1, 2))),\n",
       "                ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "fb561058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 61, 138],\n",
       "       [  1, 822]], dtype=int64)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = pipe.predict(X_test)\n",
    "confusion_matrix(y_test,y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11d5c92",
   "metadata": {},
   "source": [
    "## Grid search on TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d05fd843",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dic = {'vectorizer__max_features':[500,1000,2000,4000],\n",
    "             'vectorizer__ngram_range': [(1,1), (1,2)],\n",
    "             'vectorizer__use_idf': [False, True], # False (CountVectorizer), True (TfidfVectorizer)\n",
    "              'clf__alpha': [0.1,0.25,0.5,0.75],\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "0d926728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;background-color: white;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                        TfidfVectorizer(max_features=1000,\n",
       "                                                        ngram_range=(1, 2))),\n",
       "                                       (&#x27;clf&#x27;, MultinomialNB())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;clf__alpha&#x27;: [0.1, 0.25, 0.5, 0.75],\n",
       "                         &#x27;vectorizer__max_features&#x27;: [500, 1000, 2000, 4000],\n",
       "                         &#x27;vectorizer__ngram_range&#x27;: [(1, 1), (1, 2)],\n",
       "                         &#x27;vectorizer__use_idf&#x27;: [False, True]},\n",
       "             scoring=&#x27;roc_auc&#x27;, verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                        TfidfVectorizer(max_features=1000,\n",
       "                                                        ngram_range=(1, 2))),\n",
       "                                       (&#x27;clf&#x27;, MultinomialNB())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;clf__alpha&#x27;: [0.1, 0.25, 0.5, 0.75],\n",
       "                         &#x27;vectorizer__max_features&#x27;: [500, 1000, 2000, 4000],\n",
       "                         &#x27;vectorizer__ngram_range&#x27;: [(1, 1), (1, 2)],\n",
       "                         &#x27;vectorizer__use_idf&#x27;: [False, True]},\n",
       "             scoring=&#x27;roc_auc&#x27;, verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" ><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                 TfidfVectorizer(max_features=1000, ngram_range=(1, 2))),\n",
       "                (&#x27;clf&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" ><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_features=1000, ngram_range=(1, 2))</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" ><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('vectorizer',\n",
       "                                        TfidfVectorizer(max_features=1000,\n",
       "                                                        ngram_range=(1, 2))),\n",
       "                                       ('clf', MultinomialNB())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'clf__alpha': [0.1, 0.25, 0.5, 0.75],\n",
       "                         'vectorizer__max_features': [500, 1000, 2000, 4000],\n",
       "                         'vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
       "                         'vectorizer__use_idf': [False, True]},\n",
       "             scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid = GridSearchCV(pipe, params_dic, cv=5, n_jobs=-1, scoring='roc_auc', verbose=2)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "cb90edaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[104,  95],\n",
       "       [  4, 819]], dtype=int64)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_\n",
    "best_pipe = grid.best_estimator_\n",
    "y_test_pred = best_pipe.predict(X_test)\n",
    "confusion_matrix(y_test,y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4861eab7",
   "metadata": {},
   "source": [
    "## How does the Naive Bayes model choose 5 stars and 1 star?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "84103d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the vocabulary\n",
    "words = best_pipe['vectorizer'].get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "77cb3926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 5], dtype=int64)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe['clf'].classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c741f37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of times each word appears across all 1 star doc\n",
    "bad_word_count = best_pipe['clf'].feature_count_[0,:]\n",
    "good_word_count = best_pipe['clf'].feature_count_[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "ef9235aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>good</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00</th>\n",
       "      <td>1.747054</td>\n",
       "      <td>3.001299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>0.340837</td>\n",
       "      <td>0.576056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.759926</td>\n",
       "      <td>14.628687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10 15</th>\n",
       "      <td>0.164967</td>\n",
       "      <td>0.703823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10 min</th>\n",
       "      <td>0.530048</td>\n",
       "      <td>0.271188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yum</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.657545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yum yum</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.931869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yummy</th>\n",
       "      <td>0.173559</td>\n",
       "      <td>11.716046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zero</th>\n",
       "      <td>1.359335</td>\n",
       "      <td>0.436157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zucchini</th>\n",
       "      <td>0.045038</td>\n",
       "      <td>0.776937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               bad       good\n",
       "word                         \n",
       "00        1.747054   3.001299\n",
       "000       0.340837   0.576056\n",
       "10        5.759926  14.628687\n",
       "10 15     0.164967   0.703823\n",
       "10 min    0.530048   0.271188\n",
       "...            ...        ...\n",
       "yum       0.000000   7.657545\n",
       "yum yum   0.000000   0.931869\n",
       "yummy     0.173559  11.716046\n",
       "zero      1.359335   0.436157\n",
       "zucchini  0.045038   0.776937\n",
       "\n",
       "[4000 rows x 2 columns]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_df = pd.DataFrame({'word':words,\n",
    "                        'bad':bad_word_count,\n",
    "                         'good':good_word_count}).set_index('word')\n",
    "words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ae1aa9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 1 to the columns\n",
    "words_df = words_df + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "4ba57f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>good</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00</th>\n",
       "      <td>2.747054</td>\n",
       "      <td>4.001299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>1.340837</td>\n",
       "      <td>1.576056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6.759926</td>\n",
       "      <td>15.628687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10 15</th>\n",
       "      <td>1.164967</td>\n",
       "      <td>1.703823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10 min</th>\n",
       "      <td>1.530048</td>\n",
       "      <td>1.271188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yum</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.657545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yum yum</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.931869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yummy</th>\n",
       "      <td>1.173559</td>\n",
       "      <td>12.716046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zero</th>\n",
       "      <td>2.359335</td>\n",
       "      <td>1.436157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zucchini</th>\n",
       "      <td>1.045038</td>\n",
       "      <td>1.776937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               bad       good\n",
       "word                         \n",
       "00        2.747054   4.001299\n",
       "000       1.340837   1.576056\n",
       "10        6.759926  15.628687\n",
       "10 15     1.164967   1.703823\n",
       "10 min    1.530048   1.271188\n",
       "...            ...        ...\n",
       "yum       1.000000   8.657545\n",
       "yum yum   1.000000   1.931869\n",
       "yummy     1.173559  12.716046\n",
       "zero      2.359335   1.436157\n",
       "zucchini  1.045038   1.776937\n",
       "\n",
       "[4000 rows x 2 columns]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "36aaea6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert counts into frequencies\n",
    "words_df.bad = words_df.bad/words_df.bad.sum()\n",
    "words_df.good = words_df.good/words_df.good.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "3dc60e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratios\n",
    "words_df['bad_ratio'] = words_df.bad/words_df.good\n",
    "words_df['good_ratio'] = words_df.good/words_df.bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "57e1c67b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>good</th>\n",
       "      <th>bad_ratio</th>\n",
       "      <th>good_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>awesome</th>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.002199</td>\n",
       "      <td>0.088173</td>\n",
       "      <td>11.341296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.010377</td>\n",
       "      <td>0.092566</td>\n",
       "      <td>10.803152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>favorite</th>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.002276</td>\n",
       "      <td>0.092864</td>\n",
       "      <td>10.768386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.000717</td>\n",
       "      <td>0.006837</td>\n",
       "      <td>0.104798</td>\n",
       "      <td>9.542171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excellent</th>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.001870</td>\n",
       "      <td>0.119109</td>\n",
       "      <td>8.395654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fantastic</th>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.001159</td>\n",
       "      <td>0.120376</td>\n",
       "      <td>8.307282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perfect</th>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>0.121869</td>\n",
       "      <td>8.205523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amazing</th>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.001734</td>\n",
       "      <td>0.127226</td>\n",
       "      <td>7.860036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delicious</th>\n",
       "      <td>0.000377</td>\n",
       "      <td>0.002866</td>\n",
       "      <td>0.131585</td>\n",
       "      <td>7.599648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love place</th>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.001198</td>\n",
       "      <td>0.147254</td>\n",
       "      <td>6.790998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>friendly</th>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.002995</td>\n",
       "      <td>0.151560</td>\n",
       "      <td>6.598063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amaze</th>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.001455</td>\n",
       "      <td>0.155949</td>\n",
       "      <td>6.412360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best</th>\n",
       "      <td>0.000787</td>\n",
       "      <td>0.004959</td>\n",
       "      <td>0.158656</td>\n",
       "      <td>6.302950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>always</th>\n",
       "      <td>0.000652</td>\n",
       "      <td>0.003874</td>\n",
       "      <td>0.168430</td>\n",
       "      <td>5.937177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wonderful</th>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.001140</td>\n",
       "      <td>0.170095</td>\n",
       "      <td>5.879064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>highly recommend</th>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>0.183893</td>\n",
       "      <td>5.437934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>highly</th>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.001094</td>\n",
       "      <td>0.205938</td>\n",
       "      <td>4.855839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fresh</th>\n",
       "      <td>0.000435</td>\n",
       "      <td>0.002067</td>\n",
       "      <td>0.210522</td>\n",
       "      <td>4.750091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great food</th>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.000807</td>\n",
       "      <td>0.220503</td>\n",
       "      <td>4.535081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>definitely</th>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.001656</td>\n",
       "      <td>0.224535</td>\n",
       "      <td>4.453653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       bad      good  bad_ratio  good_ratio\n",
       "word                                                       \n",
       "awesome           0.000194  0.002199   0.088173   11.341296\n",
       "great             0.000961  0.010377   0.092566   10.803152\n",
       "favorite          0.000211  0.002276   0.092864   10.768386\n",
       "love              0.000717  0.006837   0.104798    9.542171\n",
       "excellent         0.000223  0.001870   0.119109    8.395654\n",
       "fantastic         0.000139  0.001159   0.120376    8.307282\n",
       "perfect           0.000164  0.001342   0.121869    8.205523\n",
       "amazing           0.000221  0.001734   0.127226    7.860036\n",
       "delicious         0.000377  0.002866   0.131585    7.599648\n",
       "love place        0.000176  0.001198   0.147254    6.790998\n",
       "friendly          0.000454  0.002995   0.151560    6.598063\n",
       "amaze             0.000227  0.001455   0.155949    6.412360\n",
       "best              0.000787  0.004959   0.158656    6.302950\n",
       "always            0.000652  0.003874   0.168430    5.937177\n",
       "wonderful         0.000194  0.001140   0.170095    5.879064\n",
       "highly recommend  0.000157  0.000856   0.183893    5.437934\n",
       "highly            0.000225  0.001094   0.205938    4.855839\n",
       "fresh             0.000435  0.002067   0.210522    4.750091\n",
       "great food        0.000178  0.000807   0.220503    4.535081\n",
       "definitely        0.000372  0.001656   0.224535    4.453653"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_df.sort_values(by='good_ratio', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "933bde6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Disclaimer: Like many of you, I am a sucker fo...</td>\n",
       "      <td>5</td>\n",
       "      <td>disclaimer like many sucker charm little home ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>I'm from Chicago so I'm picky with my pizza--t...</td>\n",
       "      <td>5</td>\n",
       "      <td>'m chicago 'm picky pizza -- place right ny st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>On one of my many visits to see mi amore, he t...</td>\n",
       "      <td>5</td>\n",
       "      <td>one many visit see mi amore take fantastic lit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>WOW this place is good!  SO good!  And not jus...</td>\n",
       "      <td>5</td>\n",
       "      <td>wow place good good yummy good intrinsically g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>I love this place! I love that it's to-go only...</td>\n",
       "      <td>5</td>\n",
       "      <td>love place love 's to-go 3 hour wait like pizz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>The new Harkins Cine Capri, one of the first t...</td>\n",
       "      <td>5</td>\n",
       "      <td>new harkins cine capri one first thing open te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189</th>\n",
       "      <td>I have been to this place many times and the f...</td>\n",
       "      <td>5</td>\n",
       "      <td>place many time food service always great exci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1222</th>\n",
       "      <td>Honestly, this is the best pizza that I've had...</td>\n",
       "      <td>5</td>\n",
       "      <td>honestly best pizza 've arizona 'm sucker wood...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1611</th>\n",
       "      <td>Great product! I was on a mission to make home...</td>\n",
       "      <td>5</td>\n",
       "      <td>great product mission make homemade mozzarella...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1650</th>\n",
       "      <td>Delicious food, amazing martini's, and wonderf...</td>\n",
       "      <td>5</td>\n",
       "      <td>delicious food amaze martini 's wonderful dess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1916</th>\n",
       "      <td>Stopped by here for lunch today... I saw the B...</td>\n",
       "      <td>5</td>\n",
       "      <td>stop lunch today ... saw burger sign outside t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2058</th>\n",
       "      <td>The mozzarella tomato and basil sandwich is th...</td>\n",
       "      <td>5</td>\n",
       "      <td>mozzarella tomato basil sandwich best sandwich...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2105</th>\n",
       "      <td>Yup. It's all true. Great food, super charming...</td>\n",
       "      <td>5</td>\n",
       "      <td>yup 's true great food super charm place intim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2177</th>\n",
       "      <td>Once we went to Grimaldi's in Hoboken, NJ and ...</td>\n",
       "      <td>5</td>\n",
       "      <td>go grimaldi 's hoboken nj n't impress yet firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>I found this place via Google while in town fo...</td>\n",
       "      <td>5</td>\n",
       "      <td>find place via google town business 's tuck aw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2311</th>\n",
       "      <td>Pizzeria Bianco: Before and After \\nCited as \"...</td>\n",
       "      <td>5</td>\n",
       "      <td>pizzeria bianco cite `` best pizza america '' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2683</th>\n",
       "      <td>Sauce is my kind of place.  Great, inexpensive...</td>\n",
       "      <td>5</td>\n",
       "      <td>sauce kind place great inexpensive food casual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3016</th>\n",
       "      <td>Wow... i don't think I've ever been in a more ...</td>\n",
       "      <td>5</td>\n",
       "      <td>wow ... n't think 've ever narrow place .. eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3623</th>\n",
       "      <td>Owner Daniel M. is one of my most favorite res...</td>\n",
       "      <td>5</td>\n",
       "      <td>owner daniel m. one favorite restaurateur phoe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5210</th>\n",
       "      <td>This place was great. When one of my coworkers...</td>\n",
       "      <td>5</td>\n",
       "      <td>place great one coworkers send email group vis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5398</th>\n",
       "      <td>We love 5th and Wine and recommend it to anyon...</td>\n",
       "      <td>5</td>\n",
       "      <td>love 5th wine recommend anyone ask appetizer w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5768</th>\n",
       "      <td>Really great food and service, and it worked o...</td>\n",
       "      <td>5</td>\n",
       "      <td>really great food service work fine two kid ag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6022</th>\n",
       "      <td>::Sigh::\\n\\nI love places like this.  It is ad...</td>\n",
       "      <td>5</td>\n",
       "      <td>:sigh love place like adorable unique intimate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6166</th>\n",
       "      <td>You don't expect to find a good restaurant in ...</td>\n",
       "      <td>5</td>\n",
       "      <td>n't expect find good restaurant strip mall nex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6915</th>\n",
       "      <td>Why would I go to a restaurant and hope for th...</td>\n",
       "      <td>5</td>\n",
       "      <td>would go restaurant hope bad 'm snob snob conv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7150</th>\n",
       "      <td>I love everything at Market bistro, but I espe...</td>\n",
       "      <td>5</td>\n",
       "      <td>love everything market bistro especially love ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7206</th>\n",
       "      <td>One of the best dining experiences we had in P...</td>\n",
       "      <td>5</td>\n",
       "      <td>one best dining experience phoenix surprise wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7219</th>\n",
       "      <td>5 stars? Definitely!\\n\\nUpon walking in, every...</td>\n",
       "      <td>5</td>\n",
       "      <td>5 star definitely upon walk everything atmosph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7354</th>\n",
       "      <td>When a man falls in love, it's a special thing...</td>\n",
       "      <td>5</td>\n",
       "      <td>man fall love 's special thing calloused hand ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7575</th>\n",
       "      <td>With a few hours past and a few tasks accompli...</td>\n",
       "      <td>5</td>\n",
       "      <td>hour past task accomplish next stop day would ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7813</th>\n",
       "      <td>This is by far the best farmer's market I've b...</td>\n",
       "      <td>5</td>\n",
       "      <td>far best farmer 's market 've valley still sma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8130</th>\n",
       "      <td>This place is as good as I'd heard.  We came h...</td>\n",
       "      <td>5</td>\n",
       "      <td>place good 'd hear come day-late birthday dinn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8208</th>\n",
       "      <td>Decided on a whim at the last minute to go her...</td>\n",
       "      <td>5</td>\n",
       "      <td>decide whim last minute go dinner tonight gues...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9106</th>\n",
       "      <td>Picazzo's Organic Italian Kitchen is a gem in ...</td>\n",
       "      <td>5</td>\n",
       "      <td>picazzo 's organic italian kitchen gem many pi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9201</th>\n",
       "      <td>My husband took me here on a Friday night ...a...</td>\n",
       "      <td>5</td>\n",
       "      <td>husband take friday night ... hear good thing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9389</th>\n",
       "      <td>I really like this place... \\nTaking the advic...</td>\n",
       "      <td>5</td>\n",
       "      <td>really like place ... take advice yelp stop wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9551</th>\n",
       "      <td>I love this store.  The produce is always fres...</td>\n",
       "      <td>5</td>\n",
       "      <td>love store produce always fresh interesting se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9793</th>\n",
       "      <td>I am a vegetarian and there is a ton of great ...</td>\n",
       "      <td>5</td>\n",
       "      <td>vegetarian ton great stuff pita jungle hummus ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  stars  \\\n",
       "30    Disclaimer: Like many of you, I am a sucker fo...      5   \n",
       "138   I'm from Chicago so I'm picky with my pizza--t...      5   \n",
       "205   On one of my many visits to see mi amore, he t...      5   \n",
       "452   WOW this place is good!  SO good!  And not jus...      5   \n",
       "816   I love this place! I love that it's to-go only...      5   \n",
       "913   The new Harkins Cine Capri, one of the first t...      5   \n",
       "1189  I have been to this place many times and the f...      5   \n",
       "1222  Honestly, this is the best pizza that I've had...      5   \n",
       "1611  Great product! I was on a mission to make home...      5   \n",
       "1650  Delicious food, amazing martini's, and wonderf...      5   \n",
       "1916  Stopped by here for lunch today... I saw the B...      5   \n",
       "2058  The mozzarella tomato and basil sandwich is th...      5   \n",
       "2105  Yup. It's all true. Great food, super charming...      5   \n",
       "2177  Once we went to Grimaldi's in Hoboken, NJ and ...      5   \n",
       "2221  I found this place via Google while in town fo...      5   \n",
       "2311  Pizzeria Bianco: Before and After \\nCited as \"...      5   \n",
       "2683  Sauce is my kind of place.  Great, inexpensive...      5   \n",
       "3016  Wow... i don't think I've ever been in a more ...      5   \n",
       "3623  Owner Daniel M. is one of my most favorite res...      5   \n",
       "5210  This place was great. When one of my coworkers...      5   \n",
       "5398  We love 5th and Wine and recommend it to anyon...      5   \n",
       "5768  Really great food and service, and it worked o...      5   \n",
       "6022  ::Sigh::\\n\\nI love places like this.  It is ad...      5   \n",
       "6166  You don't expect to find a good restaurant in ...      5   \n",
       "6915  Why would I go to a restaurant and hope for th...      5   \n",
       "7150  I love everything at Market bistro, but I espe...      5   \n",
       "7206  One of the best dining experiences we had in P...      5   \n",
       "7219  5 stars? Definitely!\\n\\nUpon walking in, every...      5   \n",
       "7354  When a man falls in love, it's a special thing...      5   \n",
       "7575  With a few hours past and a few tasks accompli...      5   \n",
       "7813  This is by far the best farmer's market I've b...      5   \n",
       "8130  This place is as good as I'd heard.  We came h...      5   \n",
       "8208  Decided on a whim at the last minute to go her...      5   \n",
       "9106  Picazzo's Organic Italian Kitchen is a gem in ...      5   \n",
       "9201  My husband took me here on a Friday night ...a...      5   \n",
       "9389  I really like this place... \\nTaking the advic...      5   \n",
       "9551  I love this store.  The produce is always fres...      5   \n",
       "9793  I am a vegetarian and there is a ton of great ...      5   \n",
       "\n",
       "                                         processed_text  \n",
       "30    disclaimer like many sucker charm little home ...  \n",
       "138   'm chicago 'm picky pizza -- place right ny st...  \n",
       "205   one many visit see mi amore take fantastic lit...  \n",
       "452   wow place good good yummy good intrinsically g...  \n",
       "816   love place love 's to-go 3 hour wait like pizz...  \n",
       "913   new harkins cine capri one first thing open te...  \n",
       "1189  place many time food service always great exci...  \n",
       "1222  honestly best pizza 've arizona 'm sucker wood...  \n",
       "1611  great product mission make homemade mozzarella...  \n",
       "1650  delicious food amaze martini 's wonderful dess...  \n",
       "1916  stop lunch today ... saw burger sign outside t...  \n",
       "2058  mozzarella tomato basil sandwich best sandwich...  \n",
       "2105  yup 's true great food super charm place intim...  \n",
       "2177  go grimaldi 's hoboken nj n't impress yet firs...  \n",
       "2221  find place via google town business 's tuck aw...  \n",
       "2311  pizzeria bianco cite `` best pizza america '' ...  \n",
       "2683  sauce kind place great inexpensive food casual...  \n",
       "3016  wow ... n't think 've ever narrow place .. eve...  \n",
       "3623  owner daniel m. one favorite restaurateur phoe...  \n",
       "5210  place great one coworkers send email group vis...  \n",
       "5398  love 5th wine recommend anyone ask appetizer w...  \n",
       "5768  really great food service work fine two kid ag...  \n",
       "6022  :sigh love place like adorable unique intimate...  \n",
       "6166  n't expect find good restaurant strip mall nex...  \n",
       "6915  would go restaurant hope bad 'm snob snob conv...  \n",
       "7150  love everything market bistro especially love ...  \n",
       "7206  one best dining experience phoenix surprise wi...  \n",
       "7219  5 star definitely upon walk everything atmosph...  \n",
       "7354  man fall love 's special thing calloused hand ...  \n",
       "7575  hour past task accomplish next stop day would ...  \n",
       "7813  far best farmer 's market 've valley still sma...  \n",
       "8130  place good 'd hear come day-late birthday dinn...  \n",
       "8208  decide whim last minute go dinner tonight gues...  \n",
       "9106  picazzo 's organic italian kitchen gem many pi...  \n",
       "9201  husband take friday night ... hear good thing ...  \n",
       "9389  really like place ... take advice yelp stop wa...  \n",
       "9551  love store produce always fresh interesting se...  \n",
       "9793  vegetarian ton great stuff pita jungle hummus ...  "
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find some text with 'mozzerella'\n",
    "yelp.loc[yelp.processed_text.str.contains('mozzarella')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a746b3",
   "metadata": {},
   "source": [
    "## Inspect a misclassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "c699facb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"really excite event maybe expectation high really underwhelmed little shade run water time get 1pm obligation 20 vendor food time eat way everyone else already 60 buck n't think mention `` person charge '' patronize invalidate keep tell happy chef well 'm consumer big guy sorry devour wish could well food try pretty good\""
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[(y_test==1)&(y_test_pred==5)].iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10828771",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
