{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Authorship of the Disputed Federalist Papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Federalist Papers are a collection of **85 essays** written by James Madison, Alexander Hamilton, and John Jay under the collective pseudonym \"Publius\" to promote the ratification of the United States Constitution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authorship of most of the papers were revealed some years later by Hamilton, though his claim to authorshipt of 12 papers were disputed for nearly 200 years (studies generally agree that the disputed essays were written by James Madison.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Author | Papers |\n",
    "| :- | -: | \n",
    "| Jay | 2, 3, 4, 5, 64\n",
    "| Madison | 10, 14, 37-48\n",
    "| Hamilton | 1, 6, 7, 8, 9, 11, 12, 13, 15, 16, 17, 21-36, 59, 60, 61, 65-85\n",
    "| Hamilton and Madison | 18, 19, 20\n",
    "| Disputed | 49-58, 62, 63"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this problem is to train a classifier that predicts the author of the disputed papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>To the People of the State of New York:  AFTE...</td>\n",
       "      <td>Hamilton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>To the People of the State of New York:  WHEN...</td>\n",
       "      <td>Jay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>To the People of the State of New York:  IT I...</td>\n",
       "      <td>Jay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>To the People of the State of New York:  MY L...</td>\n",
       "      <td>Jay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>To the People of the State of New York:  QUEE...</td>\n",
       "      <td>Jay</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               paper    author\n",
       "0   To the People of the State of New York:  AFTE...  Hamilton\n",
       "1   To the People of the State of New York:  WHEN...       Jay\n",
       "2   To the People of the State of New York:  IT I...       Jay\n",
       "3   To the People of the State of New York:  MY L...       Jay\n",
       "4   To the People of the State of New York:  QUEE...       Jay"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load Federalist papers data\n",
    "url = 'https://raw.githubusercontent.com/um-perez-alvaro/Data-Science-Practice/master/Data/papers.csv'\n",
    "data = pd.read_csv(url)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To the People of the State of New York:  AFTER an unequivocal experience of the inefficacy of the subsisting federal government, you are called upon to deliberate on a new Constitution for the United States of America. The subject speaks its own importance; comprehending in its consequences nothing less than the existence of the UNION, the safety and welfare of the parts of which it is composed, the fate of an empire in many respects the most interesting in the world. It has been frequently remarked that it seems to have been reserved to the people of this country, by their conduct and example, to decide the important question, whether societies of men are really capable or not of establishing good government from reflection and choice, or whether they are forever destined to depend for their political constitutions on accident and force. If there be any truth in the remark, the crisis at which we are arrived may with propriety be regarded as the era in which that decision is to be made; and a wrong election of the part we shall act may, in this view, deserve to be considered as the general misfortune of mankind.  This idea will add the inducements of philanthropy to those of patriotism, to heighten the solicitude which all considerate and good men must feel for the event. Happy will it be if our choice should be directed by a judicious estimate of our true interests, unperplexed and unbiased by considerations not connected with the public good. But this is a thing more ardently to be wished than seriously to be expected. The plan offered to our deliberations affects too many particular interests, innovates upon too many local institutions, not to involve in its discussion a variety of objects foreign to its merits, and of views, passions and prejudices little favorable to the discovery of truth.  Among the most formidable of the obstacles which the new Constitution will have to encounter may readily be distinguished the obvious interest of a certain class of men in every State to resist all changes which may hazard a diminution of the power, emolument, and consequence of the offices they hold under the State establishments; and the perverted ambition of another class of men, who will either hope to aggrandize themselves by the confusions of their country, or will flatter themselves with fairer prospects of elevation from the subdivision of the empire into several partial confederacies than from its union under one government.  It is not, however, my design to dwell upon observations of this nature. I am well aware that it would be disingenuous to resolve indiscriminately the opposition of any set of men (merely because their situations might subject them to suspicion) into interested or ambitious views. Candor will oblige us to admit that even such men may be actuated by upright intentions; and it cannot be doubted that much of the opposition which has made its appearance, or may hereafter make its appearance, will spring from sources, blameless at least, if not respectable--the honest errors of minds led astray by preconceived jealousies and fears. So numerous indeed and so powerful are the causes which serve to give a false bias to the judgment, that we, upon many occasions, see wise and good men on the wrong as well as on the right side of questions of the first magnitude to society. This circumstance, if duly attended to, would furnish a lesson of moderation to those who are ever so much persuaded of their being in the right in any controversy. And a further reason for caution, in this respect, might be drawn from the reflection that we are not always sure that those who advocate the truth are influenced by purer principles than their antagonists. Ambition, avarice, personal animosity, party opposition, and many other motives not more laudable than these, are apt to operate as well upon those who support as those who oppose the right side of a question. Were there not even these inducements to moderation, nothing could be more ill-judged than that intolerant spirit which has, at all times, characterized political parties. For in politics, as in religion, it is equally absurd to aim at making proselytes by fire and sword. Heresies in either can rarely be cured by persecution.  And yet, however just these sentiments will be allowed to be, we have already sufficient indications that it will happen in this as in all former cases of great national discussion. A torrent of angry and malignant passions will be let loose. To judge from the conduct of the opposite parties, we shall be led to conclude that they will mutually hope to evince the justness of their opinions, and to increase the number of their converts by the loudness of their declamations and the bitterness of their invectives. An enlightened zeal for the energy and efficiency of government will be stigmatized as the offspring of a temper fond of despotic power and hostile to the principles of liberty. An over-scrupulous jealousy of danger to the rights of the people, which is more commonly the fault of the head than of the heart, will be represented as mere pretense and artifice, the stale bait for popularity at the expense of the public good. It will be forgotten, on the one hand, that jealousy is the usual concomitant of love, and that the noble enthusiasm of liberty is apt to be infected with a spirit of narrow and illiberal distrust. On the other hand, it will be equally forgotten that the vigor of government is essential to the security of liberty; that, in the contemplation of a sound and well-informed judgment, their interest can never be separated; and that a dangerous ambition more often lurks behind the specious mask of zeal for the rights of the people than under the forbidden appearance of zeal for the firmness and efficiency of government. History will teach us that the former has been found a much more certain road to the introduction of despotism than the latter, and that of those men who have overturned the liberties of republics, the greatest number have begun their career by paying an obsequious court to the people; commencing demagogues, and ending tyrants.  In the course of the preceding observations, I have had an eye, my fellow-citizens, to putting you upon your guard against all attempts, from whatever quarter, to influence your decision in a matter of the utmost moment to your welfare, by any impressions other than those which may result from the evidence of truth. You will, no doubt, at the same time, have collected from the general scope of them, that they proceed from a source not unfriendly to the new Constitution. Yes, my countrymen, I own to you that, after having given it an attentive consideration, I am clearly of opinion it is your interest to adopt it. I am convinced that this is the safest course for your liberty, your dignity, and your happiness. I affect not reserves which I do not feel. I will not amuse you with an appearance of deliberation when I have decided. I frankly acknowledge to you my convictions, and I will freely lay before you the reasons on which they are founded. The consciousness of good intentions disdains ambiguity. I shall not, however, multiply professions on this head. My motives must remain in the depository of my own breast. My arguments will be open to all, and may be judged of by all. They shall at least be offered in a spirit which will not disgrace the cause of truth.  I propose, in a series of papers, to discuss the following interesting particulars:  THE UTILITY OF THE UNION TO YOUR POLITICAL PROSPERITY THE INSUFFICIENCY OF THE PRESENT CONFEDERATION TO PRESERVE THAT UNION THE NECESSITY OF A GOVERNMENT AT LEAST EQUALLY ENERGETIC WITH THE ONE PROPOSED, TO THE ATTAINMENT OF THIS OBJECT THE CONFORMITY OF THE PROPOSED CONSTITUTION TO THE TRUE PRINCIPLES OF REPUBLICAN GOVERNMENT ITS ANALOGY TO YOUR OWN STATE CONSTITUTION and lastly, THE ADDITIONAL SECURITY WHICH ITS ADOPTION WILL AFFORD TO THE PRESERVATION OF THAT SPECIES OF GOVERNMENT, TO LIBERTY, AND TO PROPERTY.  In the progress of this discussion I shall endeavor to give a satisfactory answer to all the objections which shall have made their appearance, that may seem to have any claim to your attention.  It may perhaps be thought superfluous to offer arguments to prove the utility of the UNION, a point, no doubt, deeply engraved on the hearts of the great body of the people in every State, and one, which it may be imagined, has no adversaries. But the fact is, that we already hear it whispered in the private circles of those who oppose the new Constitution, that the thirteen States are of too great extent for any general system, and that we must of necessity resort to separate confederacies of distinct portions of the whole.(1) This doctrine will, in all probability, be gradually propagated, till it has votaries enough to countenance an open avowal of it. For nothing can be more evident, to those who are able to take an enlarged view of the subject, than the alternative of an adoption of the new Constitution or a dismemberment of the Union. It will therefore be of use to begin by examining the advantages of that Union, the certain evils, and the probable dangers, to which every State will be exposed from its dissolution. This shall accordingly constitute the subject of my next address.  PUBLIUS\n"
     ]
    }
   ],
   "source": [
    "# Federalist paper No. 1\n",
    "print(data.paper[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hamilton            51\n",
       "Madison             14\n",
       "Disputed            12\n",
       "Jay                  5\n",
       "Hamilton+Madison     3\n",
       "Name: author, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.author.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1 (text processing):** remove stop words and punctuations from the papers, and lemmatize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pos(pos):\n",
    "    if pos.startswith('J'): # adjective\n",
    "        return wordnet.ADJ\n",
    "    elif pos.startswith('V'): # verb\n",
    "        return wordnet.VERB\n",
    "    elif pos.startswith('N'): # noun\n",
    "        return wordnet.NOUN\n",
    "    elif pos.startswith('R'): #adverb\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = stopwords.words('english')\n",
    "punctuation = [punc for punc in string.punctuation]\n",
    "\n",
    "def process_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    words = [word.lower() for word in words]\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word,pos=process_pos(pos))\n",
    "                       for word,pos in nltk.pos_tag(words)\n",
    "                       if word not in stop_words and word not in punctuation\n",
    "                       ]\n",
    "    return(' '.join(lemmatized_words))\n",
    "\n",
    "data['processed_paper'] = data.paper.apply(process_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2: train-test split**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the papers written by Hamilton and Madion as the training set, and the disputed papers as the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper</th>\n",
       "      <th>author</th>\n",
       "      <th>processed_paper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>To the People of the State of New York:  AFTE...</td>\n",
       "      <td>Hamilton</td>\n",
       "      <td>people state new york unequivocal experience i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>To the People of the State of New York:  WHEN...</td>\n",
       "      <td>Jay</td>\n",
       "      <td>people state new york people america reflect c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>To the People of the State of New York:  IT I...</td>\n",
       "      <td>Jay</td>\n",
       "      <td>people state new york new observation people c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               paper    author  \\\n",
       "0   To the People of the State of New York:  AFTE...  Hamilton   \n",
       "1   To the People of the State of New York:  WHEN...       Jay   \n",
       "2   To the People of the State of New York:  IT I...       Jay   \n",
       "\n",
       "                                     processed_paper  \n",
       "0  people state new york unequivocal experience i...  \n",
       "1  people state new york people america reflect c...  \n",
       "2  people state new york new observation people c...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data[data.author.isin(['Hamilton','Madison'])]\n",
    "data_test = data[data.author=='Disputed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract feature matrices X_train and X_test, and target vector y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_train.processed_paper\n",
    "X_test = data_test.processed_paper\n",
    "y_train = data_train.author\n",
    "y_test = data_test.author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Hamilton', 'Madison'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 3:** build a classification pipeline (count vectorizer + Naive Bayes model) that predicts the author of a paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB # or any other classifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                 TfidfVectorizer(max_features=1000, ngram_range=(1, 2))),\n",
       "                (&#x27;clf&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" ><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                 TfidfVectorizer(max_features=1000, ngram_range=(1, 2))),\n",
       "                (&#x27;clf&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" ><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_features=1000, ngram_range=(1, 2))</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" ><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 TfidfVectorizer(max_features=1000, ngram_range=(1, 2))),\n",
       "                ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(max_features = 1000, ngram_range=(1,2))), # idk what ngram range is. less features better for TfidVectorizer\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 4:** Use a grid search to tune the pipeline hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dic = {'vectorizer__max_features':[500,1000,2000,4000],\n",
    "             'vectorizer__ngram_range': [(1,1), (1,2)],\n",
    "             'vectorizer__use_idf': [False, True], # False (CountVectorizer), True (TfidfVectorizer)\n",
    "              'vectorizer__min_df': [1,2,4],\n",
    "              'vectorizer__max_df': [1.0,0.7,],\n",
    "              'clf__alpha': ([0.01,0.1, 0.5,0.9]),\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 384 candidates, totalling 1920 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                        TfidfVectorizer(max_features=1000,\n",
       "                                                        ngram_range=(1, 2))),\n",
       "                                       (&#x27;clf&#x27;, MultinomialNB())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;clf__alpha&#x27;: [0.01, 0.1, 0.5, 0.9],\n",
       "                         &#x27;vectorizer__max_df&#x27;: [1.0, 0.7],\n",
       "                         &#x27;vectorizer__max_features&#x27;: [500, 1000, 2000, 4000],\n",
       "                         &#x27;vectorizer__min_df&#x27;: [1, 2, 4],\n",
       "                         &#x27;vectorizer__ngram_range&#x27;: [(1, 1), (1, 2)],\n",
       "                         &#x27;vectorizer__use_idf&#x27;: [False, True]},\n",
       "             verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                                        TfidfVectorizer(max_features=1000,\n",
       "                                                        ngram_range=(1, 2))),\n",
       "                                       (&#x27;clf&#x27;, MultinomialNB())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;clf__alpha&#x27;: [0.01, 0.1, 0.5, 0.9],\n",
       "                         &#x27;vectorizer__max_df&#x27;: [1.0, 0.7],\n",
       "                         &#x27;vectorizer__max_features&#x27;: [500, 1000, 2000, 4000],\n",
       "                         &#x27;vectorizer__min_df&#x27;: [1, 2, 4],\n",
       "                         &#x27;vectorizer__ngram_range&#x27;: [(1, 1), (1, 2)],\n",
       "                         &#x27;vectorizer__use_idf&#x27;: [False, True]},\n",
       "             verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;,\n",
       "                 TfidfVectorizer(max_features=1000, ngram_range=(1, 2))),\n",
       "                (&#x27;clf&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" ><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_features=1000, ngram_range=(1, 2))</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" ><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('vectorizer',\n",
       "                                        TfidfVectorizer(max_features=1000,\n",
       "                                                        ngram_range=(1, 2))),\n",
       "                                       ('clf', MultinomialNB())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'clf__alpha': [0.01, 0.1, 0.5, 0.9],\n",
       "                         'vectorizer__max_df': [1.0, 0.7],\n",
       "                         'vectorizer__max_features': [500, 1000, 2000, 4000],\n",
       "                         'vectorizer__min_df': [1, 2, 4],\n",
       "                         'vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
       "                         'vectorizer__use_idf': [False, True]},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid = GridSearchCV(pipe, params_dic, cv=5, n_jobs=-1, verbose=2)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 5:** How does your classification model choose between Hamilton and Madison?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_\n",
    "best_pipe = grid.best_estimator_\n",
    "y_test_pred = best_pipe.predict(X_test)\n",
    "#confusion_matrix(y_test,y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Madison', 'Madison', 'Madison', 'Hamilton', 'Hamilton',\n",
       "       'Hamilton', 'Hamilton', 'Hamilton', 'Hamilton', 'Hamilton',\n",
       "       'Hamilton', 'Hamilton'], dtype='<U8')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hamilton</th>\n",
       "      <th>madison</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>able</th>\n",
       "      <td>1.880116</td>\n",
       "      <td>1.174335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abolish</th>\n",
       "      <td>1.240288</td>\n",
       "      <td>1.183015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>absolute</th>\n",
       "      <td>1.520991</td>\n",
       "      <td>1.138469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>absolutely</th>\n",
       "      <td>1.177196</td>\n",
       "      <td>1.249269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abuse</th>\n",
       "      <td>1.476452</td>\n",
       "      <td>1.066716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>writer</th>\n",
       "      <td>1.321227</td>\n",
       "      <td>1.093854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>1.650584</td>\n",
       "      <td>1.399374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yet</th>\n",
       "      <td>1.910602</td>\n",
       "      <td>1.267580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>york</th>\n",
       "      <td>2.057784</td>\n",
       "      <td>1.171113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zeal</th>\n",
       "      <td>1.340781</td>\n",
       "      <td>1.188356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            hamilton   madison\n",
       "word                          \n",
       "able        1.880116  1.174335\n",
       "abolish     1.240288  1.183015\n",
       "absolute    1.520991  1.138469\n",
       "absolutely  1.177196  1.249269\n",
       "abuse       1.476452  1.066716\n",
       "...              ...       ...\n",
       "writer      1.321227  1.093854\n",
       "year        1.650584  1.399374\n",
       "yet         1.910602  1.267580\n",
       "york        2.057784  1.171113\n",
       "zeal        1.340781  1.188356\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store the vocabulary\n",
    "words = best_pipe['vectorizer'].get_feature_names_out()\n",
    "\n",
    "# counts\n",
    "hamilton_word_count = best_pipe['clf'].feature_count_[0,:]\n",
    "madison_word_count = best_pipe['clf'].feature_count_[1,:]\n",
    "\n",
    "words_df = pd.DataFrame({'word':words,\n",
    "                        'hamilton':hamilton_word_count,\n",
    "                         'madison':madison_word_count}).set_index('word')\n",
    "words_df = words_df + 1\n",
    "words_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get words with highest use ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hamilton</th>\n",
       "      <th>madison</th>\n",
       "      <th>hamilton_ratio</th>\n",
       "      <th>madison_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>article confederation</th>\n",
       "      <td>0.000658</td>\n",
       "      <td>0.001324</td>\n",
       "      <td>0.496958</td>\n",
       "      <td>2.012244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coin</th>\n",
       "      <td>0.000615</td>\n",
       "      <td>0.001212</td>\n",
       "      <td>0.507648</td>\n",
       "      <td>1.969869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>judiciary department</th>\n",
       "      <td>0.000632</td>\n",
       "      <td>0.001228</td>\n",
       "      <td>0.514726</td>\n",
       "      <td>1.942780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>legislative department</th>\n",
       "      <td>0.000631</td>\n",
       "      <td>0.001180</td>\n",
       "      <td>0.534817</td>\n",
       "      <td>1.869798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confederation</th>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.558581</td>\n",
       "      <td>1.790251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>consequently</th>\n",
       "      <td>0.000623</td>\n",
       "      <td>0.001101</td>\n",
       "      <td>0.565988</td>\n",
       "      <td>1.766821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>department</th>\n",
       "      <td>0.000987</td>\n",
       "      <td>0.001694</td>\n",
       "      <td>0.582513</td>\n",
       "      <td>1.716701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>executive judiciary</th>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>0.589193</td>\n",
       "      <td>1.697237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>congress</th>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.001561</td>\n",
       "      <td>0.605760</td>\n",
       "      <td>1.650819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>legislative executive</th>\n",
       "      <td>0.000731</td>\n",
       "      <td>0.001193</td>\n",
       "      <td>0.612359</td>\n",
       "      <td>1.633030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>power government</th>\n",
       "      <td>0.000673</td>\n",
       "      <td>0.001067</td>\n",
       "      <td>0.630417</td>\n",
       "      <td>1.586251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>old</th>\n",
       "      <td>0.000665</td>\n",
       "      <td>0.001045</td>\n",
       "      <td>0.636648</td>\n",
       "      <td>1.570726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>violate</th>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.001063</td>\n",
       "      <td>0.648623</td>\n",
       "      <td>1.541727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>language</th>\n",
       "      <td>0.000688</td>\n",
       "      <td>0.001041</td>\n",
       "      <td>0.660690</td>\n",
       "      <td>1.513570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>definition</th>\n",
       "      <td>0.000688</td>\n",
       "      <td>0.001040</td>\n",
       "      <td>0.661715</td>\n",
       "      <td>1.511225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>absolutely</th>\n",
       "      <td>0.000691</td>\n",
       "      <td>0.001044</td>\n",
       "      <td>0.661981</td>\n",
       "      <td>1.510617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indefinite</th>\n",
       "      <td>0.000717</td>\n",
       "      <td>0.001075</td>\n",
       "      <td>0.667193</td>\n",
       "      <td>1.498816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alteration</th>\n",
       "      <td>0.000724</td>\n",
       "      <td>0.001084</td>\n",
       "      <td>0.668012</td>\n",
       "      <td>1.496980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accomplish</th>\n",
       "      <td>0.000698</td>\n",
       "      <td>0.001043</td>\n",
       "      <td>0.669110</td>\n",
       "      <td>1.494522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>executive department</th>\n",
       "      <td>0.000723</td>\n",
       "      <td>0.001077</td>\n",
       "      <td>0.671084</td>\n",
       "      <td>1.490126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        hamilton   madison  hamilton_ratio  madison_ratio\n",
       "word                                                                     \n",
       "article confederation   0.000658  0.001324        0.496958       2.012244\n",
       "coin                    0.000615  0.001212        0.507648       1.969869\n",
       "judiciary department    0.000632  0.001228        0.514726       1.942780\n",
       "legislative department  0.000631  0.001180        0.534817       1.869798\n",
       "confederation           0.000900  0.001612        0.558581       1.790251\n",
       "consequently            0.000623  0.001101        0.565988       1.766821\n",
       "department              0.000987  0.001694        0.582513       1.716701\n",
       "executive judiciary     0.000700  0.001188        0.589193       1.697237\n",
       "congress                0.000946  0.001561        0.605760       1.650819\n",
       "legislative executive   0.000731  0.001193        0.612359       1.633030\n",
       "power government        0.000673  0.001067        0.630417       1.586251\n",
       "old                     0.000665  0.001045        0.636648       1.570726\n",
       "violate                 0.000689  0.001063        0.648623       1.541727\n",
       "language                0.000688  0.001041        0.660690       1.513570\n",
       "definition              0.000688  0.001040        0.661715       1.511225\n",
       "absolutely              0.000691  0.001044        0.661981       1.510617\n",
       "indefinite              0.000717  0.001075        0.667193       1.498816\n",
       "alteration              0.000724  0.001084        0.668012       1.496980\n",
       "accomplish              0.000698  0.001043        0.669110       1.494522\n",
       "executive department    0.000723  0.001077        0.671084       1.490126"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert counts into frequencies\n",
    "words_df.hamilton = words_df.hamilton/words_df.hamilton.sum()\n",
    "words_df.madison = words_df.madison/words_df.madison.sum()\n",
    "\n",
    "# ratios\n",
    "words_df['hamilton_ratio'] = words_df.hamilton/words_df.madison\n",
    "words_df['madison_ratio'] = words_df.madison/words_df.hamilton\n",
    "\n",
    "words_df.sort_values(by='madison_ratio', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 6:** use your classifier to find who was the most likely author of the 12 disputed essays: Hamilton or Madison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Madison', 'Madison', 'Madison', 'Hamilton', 'Hamilton',\n",
       "       'Hamilton', 'Hamilton', 'Hamilton', 'Hamilton', 'Hamilton',\n",
       "       'Hamilton', 'Hamilton'], dtype='<U8')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_\n",
    "best_pipe = grid.best_estimator_\n",
    "y_test_pred = best_pipe.predict(X_test)\n",
    "y_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It predicts Madison wrote 3 while Hamilton wrote the other 9. Though other studies seem to find Madison was the author of them all. Not 100% sure why my model isn't correct. Maybe I should add more frequency parameters in gridsearch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
